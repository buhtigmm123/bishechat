{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6YY78WRU6Tl",
        "outputId": "ad666529-7e7b-4177-e1ec-4f93696ac18d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.7/dist-packages (1.13.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.37.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.21.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.44.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.17.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWabeA9kXfwg",
        "outputId": "4056a240-3027-425a-ab62-b7bd130489e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.16.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras==2.2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "vcBNWYm1YoQY",
        "outputId": "ab78c75c-ff00-44b7-a921-53a9410d9fd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 9.6 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting numpy>=1.7\n",
            "  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 59.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: six, numpy, h5py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0 numpy-1.21.5 six-1.16.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install h5py==2.10.0 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "45mQ0q5tVBX3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import operator\n",
        "\n",
        "question = np.load('pad_question.npy')\n",
        "answer = np.load('pad_answer.npy')\n",
        "answer_o = np.load('answer_o.npy',allow_pickle=True)\n",
        "with open('vocab_bag.pkl', 'rb') as f:\n",
        "    words = pickle.load(f)\n",
        "with open('pad_word_to_index.pkl', 'rb') as f:\n",
        "    word_to_index = pickle.load(f)\n",
        "with open('pad_index_to_word.pkl', 'rb') as f:\n",
        "    index_to_word = pickle.load(f)\n",
        "vocab_size = len(word_to_index) + 1\n",
        "maxLen=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8GA-auaXVa5",
        "outputId": "a0989385-0d0c-4fc4-c594-5203a2197e24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Embedding\n",
        "from keras.layers import Input, Dense, LSTM, TimeDistributed, Bidirectional, Dropout, Concatenate, RepeatVector, Activation, Dot\n",
        "from keras.layers import concatenate, dot                    \n",
        "from keras.models import Model\n",
        "#from keras.utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras.initializers import TruncatedNormal\n",
        "import keras\n",
        "import pydot\n",
        "import os, re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRuq98NsXa4X",
        "outputId": "717b1a40-f9fd-4c52-8683-64d5a8efba2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_answer (InputLayer)       (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_question (InputLayer)     (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    4335000     input_question[0][0]             \n",
            "                                                                 input_answer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "encoder_lstm (LSTM)             [(None, None, 512),  1255424     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm (LSTM)             [(None, None, 512),  1255424     embedding_1[1][0]                \n",
            "                                                                 encoder_lstm[0][1]               \n",
            "                                                                 encoder_lstm[0][2]               \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, None, None)   0           decoder_lstm[0][0]               \n",
            "                                                                 encoder_lstm[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, None)   0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dot_2 (Dot)                     (None, None, 512)    0           activation_1[0][0]               \n",
            "                                                                 encoder_lstm[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, 1024)   0           dot_2[0][0]                      \n",
            "                                                                 decoder_lstm[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, None, 256)    262400      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, None, 43350)  11140950    time_distributed_1[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 18,249,198\n",
            "Trainable params: 18,249,198\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.engine.training_utils import weighted_masked_objective\n",
        "truncatednormal = TruncatedNormal(mean=0.0, stddev=0.05)\n",
        "embed_layer = Embedding(input_dim=vocab_size, \n",
        "                        output_dim=100, \n",
        "                        mask_zero=True,\n",
        "                        input_length=None,\n",
        "                        embeddings_initializer= truncatednormal)\n",
        "LSTM_encoder = LSTM(512,\n",
        "                      return_sequences=True,\n",
        "                      return_state=True,\n",
        "                      kernel_initializer= 'lecun_uniform',\n",
        "                      name='encoder_lstm'\n",
        "                        )\n",
        "LSTM_decoder = LSTM(512, \n",
        "                    return_sequences=True, \n",
        "                    return_state=True, \n",
        "                    kernel_initializer= 'lecun_uniform',\n",
        "                    name='decoder_lstm'\n",
        "                   )\n",
        "\n",
        "#encoder输入 与 decoder输入\n",
        "input_question = Input(shape=(None, ), dtype='int32', name='input_question')\n",
        "input_answer = Input(shape=(None, ), dtype='int32', name='input_answer')\n",
        "\n",
        "input_question_embed = embed_layer(input_question)\n",
        "input_answer_embed = embed_layer(input_answer)\n",
        "\n",
        "\n",
        "encoder_lstm, question_h, question_c = LSTM_encoder(input_question_embed)\n",
        "\n",
        "decoder_lstm, _, _ = LSTM_decoder(input_answer_embed, \n",
        "                                  initial_state=[question_h, question_c])\n",
        "\n",
        "attention = dot([decoder_lstm, encoder_lstm], axes=[2, 2])\n",
        "attention = Activation('softmax')(attention)\n",
        "context = dot([attention, encoder_lstm], axes=[2,1])\n",
        "decoder_combined_context = concatenate([context, decoder_lstm])\n",
        "\n",
        "\n",
        "# Has another weight + tanh layer as described in equation (5) of the paper\n",
        "decoder_dense1 = TimeDistributed(Dense(256,activation=\"tanh\"))\n",
        "decoder_dense2 = TimeDistributed(Dense(vocab_size,activation=\"softmax\"))\n",
        "output = decoder_dense1(decoder_combined_context) # equation (5) of the paper\n",
        "output = decoder_dense2(output) # equation (6) of the paper\n",
        "\n",
        "model = Model([input_question, input_answer], output)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.load_weights('models/W--184-0.5949-.h5')\n",
        "#model = keras.models.load_model('models/W--184-0.5949-.h5')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv9y4EtpZvL_",
        "outputId": "e4fa808a-f2c8-463b-9718-e372f94a4f6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_question (InputLayer)  (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, None, 100)         4335000   \n",
            "_________________________________________________________________\n",
            "encoder_lstm (LSTM)          [(None, None, 512), (None 1255424   \n",
            "=================================================================\n",
            "Total params: 5,590,424\n",
            "Trainable params: 5,590,424\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_answer (InputLayer)       (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    4335000     input_answer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_lstm (LSTM)             [(None, None, 512),  1255424     embedding_1[1][0]                \n",
            "                                                                 input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 20, 512)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dot_3 (Dot)                     (None, None, 20)     0           decoder_lstm[1][0]               \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, 20)     0           dot_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dot_4 (Dot)                     (None, None, 512)    0           activation_2[0][0]               \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, None, 1024)   0           dot_4[0][0]                      \n",
            "                                                                 decoder_lstm[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, None, 256)    262400      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, None, 43350)  11140950    time_distributed_1[1][0]         \n",
            "==================================================================================================\n",
            "Total params: 16,993,774\n",
            "Trainable params: 16,993,774\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "question_model = Model(input_question, [encoder_lstm, question_h, question_c])\n",
        "question_model.summary()\n",
        "answer_h = Input(shape=(512,))\n",
        "answer_c = Input(shape=(512,))\n",
        "encoder_lstm = Input(shape=(maxLen,512))\n",
        "target, h, c = LSTM_decoder(input_answer_embed, initial_state=[answer_h, answer_c])\n",
        "attention = dot([target, encoder_lstm], axes=[2, 2])\n",
        "attention_ = Activation('softmax')(attention)\n",
        "context = dot([attention_, encoder_lstm], axes=[2,1])\n",
        "decoder_combined_context = concatenate([context, target])\n",
        "output = decoder_dense1(decoder_combined_context) # equation (5) of the paper\n",
        "output = decoder_dense2(output) # equation (6) of the paper\n",
        "answer_model = Model([input_answer, answer_h, answer_c, encoder_lstm], [output, h, c, attention_])\n",
        "answer_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "RicwK3JJZw6n"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import jieba\n",
        "import requests\n",
        "def act_weather(city):\n",
        "    #TODO: Get weather by api\n",
        "    url = 'http://wthrcdn.etouch.cn/weather_mini?city=' + city\n",
        "    page = requests.get(url)\n",
        "    data = page.json()\n",
        "    temperature = data['data']['wendu']\n",
        "    notice = data['data']['ganmao']\n",
        "    outstrs = \"地点： %s\\n气温： %s\\n注意： %s\" % (city, temperature, notice)\n",
        "    return outstrs + ' EOS'\n",
        "def input_question(seq):\n",
        "    seq = jieba.lcut(seq.strip(), cut_all=False)\n",
        "    sentence = seq\n",
        "    try:\n",
        "        seq = np.array([word_to_index[w] for w in seq])\n",
        "    except KeyError:\n",
        "        seq = np.array([36874, 165, 14625])\n",
        "    seq = sequence.pad_sequences([seq], maxlen=maxLen,\n",
        "                                          padding='post', truncating='post')\n",
        "    #print(seq)\n",
        "    return seq, sentence\n",
        "def decode_greedy(seq, sentence):\n",
        "    question = seq\n",
        "    for index in question[0]:\n",
        "        if int(index) == 5900:\n",
        "            for index_ in question[0]:\n",
        "                if index_ in [7851, 11842,2406, 3485, 823, 12773, 8078]:\n",
        "                    return act_weather(index_to_word[index_])\n",
        "    answer = np.zeros((1, 1))\n",
        "    attention_plot = np.zeros((20, 20))\n",
        "    answer[0, 0] = word_to_index['BOS']\n",
        "    i=1\n",
        "    answer_ = []\n",
        "    flag = 0\n",
        "    encoder_lstm_, question_h, question_c = question_model.predict(x=question, verbose=0)\n",
        "#     print(question_h, '\\n')\n",
        "    while flag != 1:\n",
        "        prediction, prediction_h, prediction_c, attention = answer_model.predict([\n",
        "            answer, question_h, question_c, encoder_lstm_\n",
        "        ])\n",
        "        attention_weights = attention.reshape(-1, )\n",
        "        attention_plot[i] = attention_weights\n",
        "        word_arg = np.argmax(prediction[0, -1, :])#\n",
        "        answer_.append(index_to_word[word_arg])\n",
        "        if word_arg == word_to_index['EOS']  or i >= 19:\n",
        "            flag = 1\n",
        "        answer = np.zeros((1, 1))\n",
        "        answer[0, 0] = word_arg\n",
        "        question_h = prediction_h\n",
        "        question_c = prediction_c\n",
        "        i += 1\n",
        "    result = ' '.join(answer_)\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence)]\n",
        "    #plot_attention(attention_plot, sentence, result.split(' '))\n",
        "    return ''.join(answer_[0:-1])\n",
        "def decode_beamsearch(seq, beam_size):\n",
        "    question = seq\n",
        "    encoder_lstm_, question_h, question_c = question_model.predict(x=question, verbose=0)\n",
        "    sequences = [[[word_to_index['BOS']], 1.0, question_h, question_c]]\n",
        "    answer = np.zeros((1, 1))\n",
        "    answer[0, 0] = word_to_index['BOS']\n",
        "    answer_ = ''\n",
        "    flag = 0\n",
        "    last_words = [word_to_index['BOS']]\n",
        "    for i in range(maxLen):\n",
        "        all_candidates = []\n",
        "        for j in range(len(sequences)):\n",
        "            s, score, h, c = sequences[j]\n",
        "            last_word = s[-1]\n",
        "            if not isinstance(last_word, int):\n",
        "                last_word=last_word[-1]\n",
        "            answer[0, 0] = last_word\n",
        "            output, h, c, _ = answer_model.predict([answer, h, c, encoder_lstm_])\n",
        "            output = output[0, -1]\n",
        "            for k in range(len(output)):\n",
        "                candidate = [seq+[k], score*-np.log(output[k]), h, c]\n",
        "            all_candidates.append(candidate)\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "        sequences = ordered[:beam_size]\n",
        "    answer_ = sequences[0][0]\n",
        "    print(answer_[0])\n",
        "    answer_ = [index_to_word[x] for x in answer_[0] if (x!=0)]\n",
        "    answer_ = ''.join(answer_)\n",
        "    return answer_\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    zhfont = matplotlib.font_manager.FontProperties(fname='simkai.ttf')\n",
        "    fig = plt.figure(figsize=(5,5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    attention = [x[::-1] for x in attention]\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    fontdict = {'fontsize': 20}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict,fontproperties=zhfont)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict, fontproperties=zhfont)\n",
        "#     ax.yaxis.set_ticks_position('right') #y轴刻度位置靠右\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "c8uzbBEqkZFg",
        "outputId": "a438af24-5da3-46ef-f743-f6330ed0ebab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "你饿不饿啊\n",
            "ANSWER:  那快去吃点东西吧别老玩QQ了\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-12cf8dd276de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    seq = input()\n",
        "    if seq == 'x':\n",
        "        break\n",
        "    seq, sentence = input_question(seq)\n",
        "    answer = decode_greedy(seq, sentence)\n",
        "    print('ANSWER: ', answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jlEf6Fnuoupn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import operator\n",
        "#main_path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "question = np.load('pad_question.npy')\n",
        "answer = np.load('pad_answer.npy')\n",
        "answer_o = np.load('answer_o.npy', allow_pickle=True)\n",
        "with open('vocab_bag.pkl', 'rb') as f:\n",
        "    words = pickle.load(f)\n",
        "with open('pad_word_to_index.pkl', 'rb') as f:\n",
        "    word_to_index = pickle.load(f)\n",
        "with open('pad_index_to_word.pkl', 'rb') as f:\n",
        "    index_to_word = pickle.load(f)\n",
        "vocab_size = len(word_to_index) + 1\n",
        "maxLen=20\n",
        "def get_file_list(file_path):\n",
        "    dir_list = os.listdir(file_path)\n",
        "    if not dir_list:\n",
        "        return\n",
        "    else:\n",
        "        dir_list = sorted(dir_list, key=lambda x: os.path.getmtime(os.path.join(file_path, x)))\n",
        "    return dir_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxkI_28epeJh",
        "outputId": "202c27ca-e9f5-4e81-b2f3-0c4148fc1797"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing import sequence\n",
        "def generate_train(batch_size):\n",
        "    print('\\n*********************************generate_train()*********************************')\n",
        "    steps=0\n",
        "    question_ = question\n",
        "    answer_ = answer\n",
        "    while True:\n",
        "        batch_answer_o = answer_o[steps:steps+batch_size]\n",
        "        batch_question = question_[steps:steps+batch_size]\n",
        "        batch_answer = answer_[steps:steps+batch_size]\n",
        "        outs = np.zeros([batch_size, maxLen, vocab_size], dtype='float32')\n",
        "        for pos, i in enumerate(batch_answer_o):\n",
        "            for pos_, j in enumerate(i):\n",
        "                if pos_ > 20:\n",
        "                    print(i)\n",
        "                outs[pos, pos_, j] = 1 # one-hot\n",
        "        yield [batch_question, batch_answer], outs\n",
        "        steps += batch_size\n",
        "        if steps == 100000:\n",
        "            steps = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "bjgo3aH_piyn",
        "outputId": "48db392d-c523-4d50-9187-8a2f89fcfa2c"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cabd3d69bd19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTruncatedNormal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_model' from 'keras.utils' (/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Embedding\n",
        "from keras.layers import Input, Dense, LSTM, TimeDistributed, Bidirectional, Dropout, Concatenate, RepeatVector, Activation, Dot\n",
        "from keras.layers import concatenate, dot                    \n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard,ReduceLROnPlateau\n",
        "from keras.initializers import TruncatedNormal\n",
        "import pydot\n",
        "import os, re\n",
        "truncatednormal = TruncatedNormal(mean=0.0, stddev=0.05)\n",
        "embed_layer = Embedding(input_dim=vocab_size, \n",
        "                        output_dim=100, \n",
        "                        mask_zero=True,\n",
        "                        input_length=None,\n",
        "                        embeddings_initializer= truncatednormal)\n",
        "# embed_layer.build((None,))\n",
        "\n",
        "LSTM_encoder = LSTM(512,\n",
        "                      return_sequences=True,\n",
        "                      return_state=True,\n",
        "#                       activation='relu',\n",
        "#                       dropout=0.25,\n",
        "#                       recurrent_dropout=0.1,\n",
        "                      kernel_initializer= 'lecun_uniform',\n",
        "                      name='encoder_lstm'\n",
        "                        )\n",
        "LSTM_decoder = LSTM(512, \n",
        "                    return_sequences=True, \n",
        "                    return_state=True, \n",
        "#                     activation = 'relu',\n",
        "#                     dropout=0.25, \n",
        "#                     recurrent_dropout=0.1,\n",
        "                    kernel_initializer= 'lecun_uniform',\n",
        "                    name='decoder_lstm'\n",
        "                   )\n",
        "\n",
        "#encoder输入 与 decoder输入\n",
        "input_question = Input(shape=(None, ), dtype='int32', name='input_question')\n",
        "input_answer = Input(shape=(None, ), dtype='int32', name='input_answer')\n",
        "input_question_embed = embed_layer(input_question)\n",
        "input_answer_embed = embed_layer(input_answer)\n",
        "\n",
        "\n",
        "encoder_lstm, question_h, question_c = LSTM_encoder(input_question_embed)\n",
        "\n",
        "decoder_lstm, _, _ = LSTM_decoder(input_answer_embed, \n",
        "                                  initial_state=[question_h, question_c])\n",
        "\n",
        "attention = dot([decoder_lstm, encoder_lstm], axes=[2, 2])\n",
        "attention = Activation('softmax')(attention)\n",
        "context = dot([attention, encoder_lstm], axes=[2,1])\n",
        "decoder_combined_context = concatenate([context, decoder_lstm])\n",
        "\n",
        "# output = dense1(decoder_combined_context)\n",
        "# output = dense2(Dropout(0.5)(output))\n",
        "\n",
        "# Has another weight + tanh layer as described in equation (5) of the paper\n",
        "decoder_dense1 = TimeDistributed(Dense(256,activation=\"tanh\"))\n",
        "decoder_dense2 = TimeDistributed(Dense(vocab_size,activation=\"softmax\"))\n",
        "output = decoder_dense1(decoder_combined_context) # equation (5) of the paper\n",
        "output = decoder_dense2(output) # equation (6) of the paper\n",
        "\n",
        "model = Model([input_question, input_answer], output)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "filepath = \"modles/W-\" + \"-{epoch:3d}-{loss:.4f}-.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath,\n",
        "                             monitor='loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='min',\n",
        "                             period=1,\n",
        "                             save_weights_only=True\n",
        "                             )\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', \n",
        "                              factor=0.2, \n",
        "                              patience=2, \n",
        "                              verbose=1, \n",
        "                              mode='min', \n",
        "                              min_delta=0.0001, \n",
        "                              cooldown=0, \n",
        "                              min_lr=0\n",
        "                              )\n",
        "tensorboard = TensorBoard(log_dir='logs', \n",
        "#                           histogram_freq=0, \n",
        "                          batch_size=100\n",
        "#                           write_graph=True, \n",
        "#                           write_grads=True, \n",
        "#                           write_images=True, \n",
        "#                           embeddings_freq=0, \n",
        "#                           embeddings_layer_names=None, \n",
        "#                           embeddings_metadata=None, \n",
        "#                           embeddings_data=None, \n",
        "#                           update_freq='epoch'\n",
        "                         )\n",
        "callbacks_list = [checkpoint, reduce_lr, tensorboard]\n",
        "initial_epoch_=0\n",
        "#file_list = os.listdir('modles/')\n",
        "#if len(file_list) > 0:\n",
        "#    epoch_list = get_file_list('modles/')\n",
        "#    epoch_last = epoch_list[-1]\n",
        "#    model.load_weights('modles/' + epoch_last)\n",
        "#    print(\"**********checkpoint_loaded: \", epoch_last)\n",
        "#    initial_epoch_ = int(epoch_last.split('-')[2]) - 1\n",
        "#    print('**********Begin from epoch: ', str(initial_epoch_))\n",
        "\n",
        "model.fit_generator(generate_train(batch_size=100), \n",
        "                    steps_per_epoch=1000, # (total samples) / batch_size 100000/100 = 1000\n",
        "                    epochs=200, \n",
        "                    verbose=1, \n",
        "                    callbacks=callbacks_list, \n",
        "#                     validation_data=generate_test(batch_size=100), \n",
        "#                     validation_steps=200, # 10000/100 = 100\n",
        "                    class_weight=None, \n",
        "                    max_queue_size=5, \n",
        "                    workers=1, \n",
        "                    use_multiprocessing=False, \n",
        "                    shuffle=False, \n",
        "                    initial_epoch=initial_epoch_\n",
        "                    )\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6EtMugc2IWk"
      },
      "outputs": [],
      "source": [
        "question_model = Model(input_question, [encoder_lstm, question_h, question_c])\n",
        "question_model.summary()\n",
        "answer_h = Input(shape=(512,))\n",
        "answer_c = Input(shape=(512,))\n",
        "encoder_lstm = Input(shape=(maxLen,512))\n",
        "target, h, c = LSTM_decoder(input_answer_embed, initial_state=[answer_h, answer_c])\n",
        "attention = dot([target, encoder_lstm], axes=[2, 2])\n",
        "attention_ = Activation('softmax')(attention)\n",
        "context = dot([attention_, encoder_lstm], axes=[2,1])\n",
        "decoder_combined_context = concatenate([context, target])\n",
        "output = decoder_dense1(decoder_combined_context) # equation (5) of the paper\n",
        "output = decoder_dense2(output) # equation (6) of the paper\n",
        "answer_model = Model([input_answer, answer_h, answer_c, encoder_lstm], [output, h, c, attention_])\n",
        "answer_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URHc4LiV3ZDM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
